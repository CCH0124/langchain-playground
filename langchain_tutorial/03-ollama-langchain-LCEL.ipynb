{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761334f4",
   "metadata": {},
   "source": [
    "## LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176379e",
   "metadata": {},
   "source": [
    "1. 所有元件皆實作 Runnable 協定\n",
    "\n",
    "| 方法        | 功能                   |\n",
    "| --------- | -------------------- |\n",
    "| `invoke`  | 同步呼叫                 |\n",
    "| `ainvoke` | 非同步呼叫                |\n",
    "| `stream`  | 串流輸出（token by token） |\n",
    "| `astream` | 非同步串流輸出              |\n",
    "| `batch`   | 一次處理多組輸入             |\n",
    "| `abatch`  | 非同步 batch 處理         |\n",
    "\n",
    "2. 通用屬性（Common Properties）：\n",
    "- input_schema: 元件可以接受的輸入格式\n",
    "- output_schema: 元件輸出的格式結構\n",
    "\n",
    "3. 常見 LCEL 元件與 I/O 對照表\n",
    "\n",
    "| Component      | Input Type                             | Output Type          |\n",
    "| -------------- | -------------------------------------- | -------------------- |\n",
    "| `Prompt`       | Dictionary                             | Prompt Value（LLM 可用） |\n",
    "| `Retriever`    | Single String（如 query）                 | List of Documents    |\n",
    "| `LLM`          | String, List of Messages, Prompt Value | String               |\n",
    "| `ChatModel`    | String, List of Messages, Prompt Value | ChatMessage          |\n",
    "| `Tool`         | String / Dictionary                    | 依 tool 定義而異          |\n",
    "| `OutputParser` | LLM 或 ChatModel 的 Output               | 視 parser 而定          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc9cb7",
   "metadata": {},
   "source": [
    "`Runnables` 支援\n",
    "1. \n",
    "- Async：支援非同步 ainvoke() 執行，適用於等待外部 API 或 LLM 的情境。\n",
    "- Batch：可一次處理多筆輸入（使用 batch() 或 abatch()）。\n",
    "- Streaming：支援串流回應（例如 LLM token-by-token 的輸出）。\n",
    "\n",
    "2. Fallbacks\n",
    "\n",
    "提供「備援處理機制」，例如：\n",
    "\n",
    "- 如果主模型失敗，使用備用模型。\n",
    "- 如果某一步驟錯誤，退回預設輸出。\n",
    "- 可以用在 `.with_fallbacks([r1, r2])` 等操作中\n",
    "\n",
    "3. Parallelism（並行處理）\n",
    "\n",
    "設計考量到 LLM 呼叫成本高、耗時。可以並行處理多個步驟：\n",
    "\n",
    "- 如多個 Retriever、Tool、Chain。\n",
    "- 透過 RunnableParallel, RunnableMap, RunnableSequence 等組件實現。\n",
    "\n",
    "4. Logging\n",
    "- 已內建追蹤與記錄功能（整合 langchain.debug, langsmith, logging）。\n",
    "- 可用來檢查 Chain 執行細節、錯誤或效能瓶頸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "chat = OllamaLLM(temperature=0.4, model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"summary\", description=\"Summary of the input question\"),\n",
    "    ResponseSchema(name=\"category\", description=\"One-word category of the question (e.g., math, general, science)\")\n",
    "]\n",
    "\n",
    "# 2. 建立 JSON output parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5567bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"summary\": string  // Summary of the input question\\n\\t\"category\": string  // One-word category of the question (e.g., math, general, science)\\n}\\n```'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'question'], input_types={}, partial_variables={}, template='\\nYou will be given a question. Your job is to return a JSON object with:\\n- a \"summary\" of the question\\n- a \"category\" for the question\\n\\nQuestion: {question}\\n\\n{format_instructions}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will be given a question. Your job is to return a JSON object with:\n",
    "- a \"summary\" of the question\n",
    "- a \"category\" for the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': \"The reason for the sky's blue color\", 'category': 'science'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(\n",
    "    prompt,\n",
    "    chat,\n",
    "    chat\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"question\": \"Why is the sky blue?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054c6b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Basic arithmetic', 'category': 'math'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = prompt | chat | parser\n",
    "\n",
    "sequence.invoke({\"question\": \"What is 1+1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd11e3d",
   "metadata": {},
   "source": [
    "### More complex chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3425ae07",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itachi/.local/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(temperature=0.0, model=\"llama3.2\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"kube-apiserver: The core component server that exposes the Kubernetes HTTP API\", \"etcd: Consistent and highly-available key value store for all API server data\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd6efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='kube-apiserver: The core component server that exposes the Kubernetes HTTP API'),\n",
       " Document(metadata={}, page_content='etcd: Consistent and highly-available key value store for all API server data')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is kube-apiserver?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3a8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='etcd: Consistent and highly-available key value store for all API server data'),\n",
       " Document(metadata={}, page_content='kube-apiserver: The core component server that exposes the Kubernetes HTTP API')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is etcd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bae0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the question based only on the following context: {context}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636bfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcc77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82aa5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(...),\n",
       "  question: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the question based only on the following context: {context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2', temperature=0.0)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9640080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kube-apiserver is the core component of a Kubernetes cluster. It's responsible for exposing the Kubernetes REST API, which allows users to interact with the cluster and manage its resources, such as pods, services, and deployments. In other words, it acts as an entry point for the Kubernetes cluster, providing a interface for clients (such as the Kubernetes CLI or other applications) to access and manipulate cluster data.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is kube-apiserver?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc857d7",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0289ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = ChatPromptTemplate.from_template(\"Summarize this: {text}\")\n",
    "prompt_translate = ChatPromptTemplate.from_template(\"Translate into Chinese Tranditional: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f915e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "summary_chain = prompt_summary | chat | StrOutputParser()\n",
    "translate_chain = prompt_translate | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2990c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary: LangChain is a robust tool, but it requires a significant amount of effort and dedication to learn how to use it effectively.\n",
      "✅ Translate: Here's the translation:\n",
      "\n",
      "人工：LangChain 是 一個強大框架，但具有陡峭的學習曲線。\n",
      "\n",
      "Traditional Chinese:\n",
      "\n",
      "\n",
      "\n",
      "Note: \"\" means \"steep\" or \"sharp\", and \"\" means \"learning curve\". The phrase \"\" is a common idiomatic expression in Chinese that refers to something having a difficult learning process.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    summarize=summary_chain,\n",
    "    translate=translate_chain\n",
    ")\n",
    "\n",
    "# 5. 執行\n",
    "input_text = {\"text\": \"LangChain is a powerful framework but has a steep learning curve.\"}\n",
    "result = parallel_chain.invoke(input_text)\n",
    "\n",
    "print(\"✅ Summary:\", result[\"summarize\"])\n",
    "print(\"✅ Translate:\", result[\"translate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06320a7",
   "metadata": {},
   "source": [
    "## RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ab9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Prompt: 一般問題\n",
    "general_prompt = ChatPromptTemplate.from_template(\"Answer casually: {question}\")\n",
    "general_chain = general_prompt | chat | () | RunnableLambda(lambda output: {\n",
    "        \"source\": \"general\",\n",
    "        \"result\": output\n",
    "    })\n",
    "# Prompt: 數學問題\n",
    "math_prompt = ChatPromptTemplate.from_template(\"You're a math tutor. Solve step by step: {question}\")\n",
    "math_chain = math_prompt | chat | StrOutputParser() | RunnableLambda(lambda output: {\n",
    "    \"source\": \"math\",\n",
    "    \"result\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "452a10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"math\" in x[\"question\"].lower(), math_chain),\n",
    "    general_chain  # 預設 fallback branch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af44f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️ General: {'source': 'general', 'result': \"You know, it's just one of those things that's always been there, right? But if you really want to get into it, I guess it has something to do with this thing called light scattering. See, when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases like nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\\n\\nSo, when we look up at the sky, our eyes see the combined effect of all that scattered blue light, making it appear blue to us! It's pretty cool, actually. And if you've ever seen a sunset or sunrise, you know that the color of the sky can change depending on the time of day and the amount of dust and water vapor in the air. But basically, the blue sky is just our atmosphere doing its thing!\\n\\nWhat do you think? Want to go stargazing tonight?\"}\n"
     ]
    }
   ],
   "source": [
    "# 測試一：一般問題\n",
    "res1 = branch.invoke({\"question\": \"Why is the sky blue?\"})\n",
    "print(\"☁️ General:\", res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c67368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 Math: {'source': 'math', 'result': \"I'd be happy to help you solve the equation 2x + 3 = 7.\\n\\nTo start, we can subtract 3 from both sides of the equation to get:\\n\\n2x = 7 - 3\\n2x = 4\\n\\nNext, we can divide both sides of the equation by 2 to solve for x. This gives us:\\n\\nx = 4/2\\nx = 2\\n\\nSo, the value of x is 2!\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 測試二：數學問題\n",
    "res2 = branch.invoke({\"question\": \"How to solve this math equation: 2x + 3 = 7?\"})\n",
    "print(\"📐 Math:\", res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
