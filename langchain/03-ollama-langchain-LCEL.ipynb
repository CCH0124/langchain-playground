{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761334f4",
   "metadata": {},
   "source": [
    "## LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176379e",
   "metadata": {},
   "source": [
    "1. æ‰€æœ‰å…ƒä»¶çš†å¯¦ä½œ Runnable å”å®š\n",
    "\n",
    "| æ–¹æ³•        | åŠŸèƒ½                   |\n",
    "| --------- | -------------------- |\n",
    "| `invoke`  | åŒæ­¥å‘¼å«                 |\n",
    "| `ainvoke` | éåŒæ­¥å‘¼å«                |\n",
    "| `stream`  | ä¸²æµè¼¸å‡ºï¼ˆtoken by tokenï¼‰ |\n",
    "| `astream` | éåŒæ­¥ä¸²æµè¼¸å‡º              |\n",
    "| `batch`   | ä¸€æ¬¡è™•ç†å¤šçµ„è¼¸å…¥             |\n",
    "| `abatch`  | éåŒæ­¥ batch è™•ç†         |\n",
    "\n",
    "2. é€šç”¨å±¬æ€§ï¼ˆCommon Propertiesï¼‰ï¼š\n",
    "- input_schema: å…ƒä»¶å¯ä»¥æ¥å—çš„è¼¸å…¥æ ¼å¼\n",
    "- output_schema: å…ƒä»¶è¼¸å‡ºçš„æ ¼å¼çµæ§‹\n",
    "\n",
    "3. å¸¸è¦‹ LCEL å…ƒä»¶èˆ‡ I/O å°ç…§è¡¨\n",
    "\n",
    "| Component      | Input Type                             | Output Type          |\n",
    "| -------------- | -------------------------------------- | -------------------- |\n",
    "| `Prompt`       | Dictionary                             | Prompt Valueï¼ˆLLM å¯ç”¨ï¼‰ |\n",
    "| `Retriever`    | Single Stringï¼ˆå¦‚ queryï¼‰                 | List of Documents    |\n",
    "| `LLM`          | String, List of Messages, Prompt Value | String               |\n",
    "| `ChatModel`    | String, List of Messages, Prompt Value | ChatMessage          |\n",
    "| `Tool`         | String / Dictionary                    | ä¾ tool å®šç¾©è€Œç•°          |\n",
    "| `OutputParser` | LLM æˆ– ChatModel çš„ Output               | è¦– parser è€Œå®š          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc9cb7",
   "metadata": {},
   "source": [
    "`Runnables` æ”¯æ´\n",
    "1. \n",
    "- Asyncï¼šæ”¯æ´éåŒæ­¥ ainvoke() åŸ·è¡Œï¼Œé©ç”¨æ–¼ç­‰å¾…å¤–éƒ¨ API æˆ– LLM çš„æƒ…å¢ƒã€‚\n",
    "- Batchï¼šå¯ä¸€æ¬¡è™•ç†å¤šç­†è¼¸å…¥ï¼ˆä½¿ç”¨ batch() æˆ– abatch()ï¼‰ã€‚\n",
    "- Streamingï¼šæ”¯æ´ä¸²æµå›æ‡‰ï¼ˆä¾‹å¦‚ LLM token-by-token çš„è¼¸å‡ºï¼‰ã€‚\n",
    "\n",
    "2. Fallbacks\n",
    "\n",
    "æä¾›ã€Œå‚™æ´è™•ç†æ©Ÿåˆ¶ã€ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- å¦‚æœä¸»æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ¨¡å‹ã€‚\n",
    "- å¦‚æœæŸä¸€æ­¥é©ŸéŒ¯èª¤ï¼Œé€€å›é è¨­è¼¸å‡ºã€‚\n",
    "- å¯ä»¥ç”¨åœ¨ `.with_fallbacks([r1, r2])` ç­‰æ“ä½œä¸­\n",
    "\n",
    "3. Parallelismï¼ˆä¸¦è¡Œè™•ç†ï¼‰\n",
    "\n",
    "è¨­è¨ˆè€ƒé‡åˆ° LLM å‘¼å«æˆæœ¬é«˜ã€è€—æ™‚ã€‚å¯ä»¥ä¸¦è¡Œè™•ç†å¤šå€‹æ­¥é©Ÿï¼š\n",
    "\n",
    "- å¦‚å¤šå€‹ Retrieverã€Toolã€Chainã€‚\n",
    "- é€é RunnableParallel, RunnableMap, RunnableSequence ç­‰çµ„ä»¶å¯¦ç¾ã€‚\n",
    "\n",
    "4. Logging\n",
    "- å·²å…§å»ºè¿½è¹¤èˆ‡è¨˜éŒ„åŠŸèƒ½ï¼ˆæ•´åˆ langchain.debug, langsmith, loggingï¼‰ã€‚\n",
    "- å¯ç”¨ä¾†æª¢æŸ¥ Chain åŸ·è¡Œç´°ç¯€ã€éŒ¯èª¤æˆ–æ•ˆèƒ½ç“¶é ¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "chat = OllamaLLM(temperature=0.4, model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"summary\", description=\"Summary of the input question\"),\n",
    "    ResponseSchema(name=\"category\", description=\"One-word category of the question (e.g., math, general, science)\")\n",
    "]\n",
    "\n",
    "# 2. å»ºç«‹ JSON output parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5567bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"summary\": string  // Summary of the input question\\n\\t\"category\": string  // One-word category of the question (e.g., math, general, science)\\n}\\n```'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'question'], input_types={}, partial_variables={}, template='\\nYou will be given a question. Your job is to return a JSON object with:\\n- a \"summary\" of the question\\n- a \"category\" for the question\\n\\nQuestion: {question}\\n\\n{format_instructions}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will be given a question. Your job is to return a JSON object with:\n",
    "- a \"summary\" of the question\n",
    "- a \"category\" for the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': \"The reason for the sky's blue color\", 'category': 'science'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(\n",
    "    prompt,\n",
    "    chat,\n",
    "    chat\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"question\": \"Why is the sky blue?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054c6b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Basic arithmetic', 'category': 'math'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = prompt | chat | parser\n",
    "\n",
    "sequence.invoke({\"question\": \"What is 1+1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd11e3d",
   "metadata": {},
   "source": [
    "### More complex chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3425ae07",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itachi/.local/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(temperature=0.0, model=\"llama3.2\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"kube-apiserver: The core component server that exposes the Kubernetes HTTP API\", \"etcd: Consistent and highly-available key value store for all API server data\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd6efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='kube-apiserver: The core component server that exposes the Kubernetes HTTP API'),\n",
       " Document(metadata={}, page_content='etcd: Consistent and highly-available key value store for all API server data')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is kube-apiserver?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3a8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='etcd: Consistent and highly-available key value store for all API server data'),\n",
       " Document(metadata={}, page_content='kube-apiserver: The core component server that exposes the Kubernetes HTTP API')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is etcd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bae0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the question based only on the following context: {context}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636bfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcc77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82aa5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(...),\n",
       "  question: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the question based only on the following context: {context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2', temperature=0.0)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9640080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kube-apiserver is the core component of a Kubernetes cluster. It's responsible for exposing the Kubernetes REST API, which allows users to interact with the cluster and manage its resources, such as pods, services, and deployments. In other words, it acts as an entry point for the Kubernetes cluster, providing a interface for clients (such as the Kubernetes CLI or other applications) to access and manipulate cluster data.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is kube-apiserver?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc857d7",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0289ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = ChatPromptTemplate.from_template(\"Summarize this: {text}\")\n",
    "prompt_translate = ChatPromptTemplate.from_template(\"Translate into Chinese Tranditional: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f915e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "summary_chain = prompt_summary | chat | StrOutputParser()\n",
    "translate_chain = prompt_translate | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2990c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary: LangChain is a robust tool, but it requires a significant amount of effort and dedication to learn how to use it effectively.\n",
      "âœ… Translate: Here's the translation:\n",
      "\n",
      "äººå·¥ï¼šLangChain æ˜¯ ä¸€å€‹å¼·å¤§æ¡†æ¶ï¼Œä½†å…·æœ‰é™¡å³­çš„å­¸ç¿’æ›²ç·šã€‚\n",
      "\n",
      "Traditional Chinese:\n",
      "\n",
      "\n",
      "\n",
      "Note: \"\" means \"steep\" or \"sharp\", and \"\" means \"learning curve\". The phrase \"\" is a common idiomatic expression in Chinese that refers to something having a difficult learning process.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    summarize=summary_chain,\n",
    "    translate=translate_chain\n",
    ")\n",
    "\n",
    "# 5. åŸ·è¡Œ\n",
    "input_text = {\"text\": \"LangChain is a powerful framework but has a steep learning curve.\"}\n",
    "result = parallel_chain.invoke(input_text)\n",
    "\n",
    "print(\"âœ… Summary:\", result[\"summarize\"])\n",
    "print(\"âœ… Translate:\", result[\"translate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06320a7",
   "metadata": {},
   "source": [
    "## RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ab9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Prompt: ä¸€èˆ¬å•é¡Œ\n",
    "general_prompt = ChatPromptTemplate.from_template(\"Answer casually: {question}\")\n",
    "general_chain = general_prompt | chat | () | RunnableLambda(lambda output: {\n",
    "        \"source\": \"general\",\n",
    "        \"result\": output\n",
    "    })\n",
    "# Prompt: æ•¸å­¸å•é¡Œ\n",
    "math_prompt = ChatPromptTemplate.from_template(\"You're a math tutor. Solve step by step: {question}\")\n",
    "math_chain = math_prompt | chat | StrOutputParser() | RunnableLambda(lambda output: {\n",
    "    \"source\": \"math\",\n",
    "    \"result\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "452a10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"math\" in x[\"question\"].lower(), math_chain),\n",
    "    general_chain  # é è¨­ fallback branch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af44f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â˜ï¸ General: {'source': 'general', 'result': \"You know, it's just one of those things that's always been there, right? But if you really want to get into it, I guess it has something to do with this thing called light scattering. See, when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases like nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\\n\\nSo, when we look up at the sky, our eyes see the combined effect of all that scattered blue light, making it appear blue to us! It's pretty cool, actually. And if you've ever seen a sunset or sunrise, you know that the color of the sky can change depending on the time of day and the amount of dust and water vapor in the air. But basically, the blue sky is just our atmosphere doing its thing!\\n\\nWhat do you think? Want to go stargazing tonight?\"}\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦ä¸€ï¼šä¸€èˆ¬å•é¡Œ\n",
    "res1 = branch.invoke({\"question\": \"Why is the sky blue?\"})\n",
    "print(\"â˜ï¸ General:\", res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c67368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Math: {'source': 'math', 'result': \"I'd be happy to help you solve the equation 2x + 3 = 7.\\n\\nTo start, we can subtract 3 from both sides of the equation to get:\\n\\n2x = 7 - 3\\n2x = 4\\n\\nNext, we can divide both sides of the equation by 2 to solve for x. This gives us:\\n\\nx = 4/2\\nx = 2\\n\\nSo, the value of x is 2!\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# æ¸¬è©¦äºŒï¼šæ•¸å­¸å•é¡Œ\n",
    "res2 = branch.invoke({\"question\": \"How to solve this math equation: 2x + 3 = 7?\"})\n",
    "print(\"ğŸ“ Math:\", res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
